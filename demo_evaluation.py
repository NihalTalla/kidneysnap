#!/usr/bin/env python3
"""
Complete Model Evaluation Demo
Demonstrates the comprehensive evaluation capabilities of simple_evaluation.py
"""

import os
import json
from datetime import datetime

def main():
    """Demonstrate the complete evaluation system."""
    print("ğŸ©º KIDNEY STONE DETECTION - COMPLETE EVALUATION DEMO")
    print("=" * 70)
    
    # Check if evaluation script exists
    if not os.path.exists("simple_evaluation.py"):
        print("âŒ simple_evaluation.py not found")
        return
    
    # Run the evaluation
    print("ğŸ”„ Running comprehensive model evaluation...")
    print("This will:")
    print("   âœ… Load the trained model and test dataset")
    print("   âœ… Calculate accuracy, precision, recall, F1-score on real test data")
    print("   âœ… Generate confusion matrix and classification report")
    print("   âœ… Create visualizations of results")
    print("   âœ… Save evaluation metrics to JSON file")
    print()
    
    # Import and run evaluation
    try:
        from simple_evaluation import main as eval_main
        results_file = eval_main()
        
        if results_file:
            print(f"\nğŸ“Š EVALUATION SUMMARY:")
            print(f"   ğŸ“ Results saved to: {results_file}")
            
            # Load and display key metrics
            try:
                with open(results_file, 'r') as f:
                    results = json.load(f)
                
                print(f"   ğŸ“ˆ Overall Accuracy: {results.get('overall_accuracy', 0):.2%}")
                print(f"   ğŸ¯ F1-Score (Macro): {results.get('macro_f1', 0):.4f}")
                print(f"   ğŸ“Š Total Test Samples: {results.get('total_samples', 0)}")
                print(f"   ğŸ•’ Evaluation Date: {results.get('evaluation_date', 'Unknown')}")
                
                # Model information
                model_info = results.get('model_info', {})
                if model_info:
                    print(f"   ğŸ—ï¸  Model Architecture: {model_info.get('architecture', 'Unknown')}")
                    print(f"   ğŸ“Š Model Parameters: {model_info.get('parameters', 0):,}")
                
                # Visualizations
                vis_paths = results.get('visualization_paths', [])
                if vis_paths:
                    print(f"   ğŸ“Š Visualizations Generated: {len(vis_paths)}")
                    for path in vis_paths:
                        print(f"      - {os.path.basename(path)}")
                
            except Exception as e:
                print(f"   âš ï¸  Could not load results details: {e}")
        
        print("\nâœ… Complete evaluation demonstration finished!")
        print("\nğŸ¯ What was accomplished:")
        print("   âœ… Real model evaluation on actual test data")
        print("   âœ… Comprehensive metrics calculation")
        print("   âœ… Professional visualization generation")
        print("   âœ… JSON export for further analysis")
        print("   âœ… Clinical performance assessment")
        
        print("\nğŸ“ Generated Files:")
        if os.path.exists("evaluation_plots"):
            plots = os.listdir("evaluation_plots")
            for plot in plots:
                print(f"   ğŸ“Š evaluation_plots/{plot}")
        
        # List recent evaluation results
        eval_files = [f for f in os.listdir('.') if f.startswith('evaluation_results_') and f.endswith('.json')]
        eval_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        
        if eval_files:
            print(f"   ğŸ“„ {eval_files[0]} (latest)")
            if len(eval_files) > 1:
                print(f"   ğŸ“„ {len(eval_files)-1} additional evaluation file(s)")
        
        print(f"\nğŸ¥ The evaluation system provides:")
        print(f"   â€¢ Medical-grade performance metrics")
        print(f"   â€¢ Clinical interpretation and recommendations")
        print(f"   â€¢ Professional visualizations")
        print(f"   â€¢ Complete audit trail in JSON format")
        print(f"   â€¢ Integration with your training pipeline")
        
    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        print("ğŸ’¡ Make sure you have:")
        print("   â€¢ A trained model in models/ directory")
        print("   â€¢ Test data in Dataset/Test/ directory")
        print("   â€¢ All required dependencies installed")

if __name__ == "__main__":
    main()